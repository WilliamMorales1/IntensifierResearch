{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845927b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run before everything else\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main data xlsx maker\n",
    "def create_data(input_path, output_filename):\n",
    "    INPUT_FOLDER_NAME = input_path + \"\\\\tagged\"\n",
    "    OUTPUT_FILE_NAME = input_path + \"\\\\\" + output_filename + \".csv\"\n",
    "\n",
    "    def remove_SPACE():\n",
    "        # Check if the directory exists\n",
    "        if os.path.isdir(INPUT_FOLDER_NAME):\n",
    "            # List all files in the directory\n",
    "            files = os.listdir(INPUT_FOLDER_NAME)\n",
    "\n",
    "            # Loop through each file\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.txt'):\n",
    "                    file_path = os.path.join(INPUT_FOLDER_NAME, file_name)\n",
    "\n",
    "                    # Read the content of the file with the detected encoding\n",
    "                    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "                        content = file.read()\n",
    "\n",
    "                    # Replace _SPACE_ with an empty string\n",
    "                    modified_content = content.replace('_SPACE_', '')\n",
    "\n",
    "                    # Write the modified content back to the file\n",
    "                    with open(file_path, 'w', encoding=\"utf-8\") as file:\n",
    "                        file.write(modified_content)\n",
    "\n",
    "    def analyze(input_folder):\n",
    "\n",
    "        # no mas, tan\n",
    "        intensifier_list = [\"absolutamente\", \"bastante\", \"casi\", \"completamente\", \"demasiado\", \"estrictamente\",\n",
    "                \"especialmente\", \"extremadamente\", \"harto\", \"increíblemente\", \"mucho\",\n",
    "                \"muy\", \"plenamente\", \"realmente\", \"sumamente\", \"súper\", \"totalmente\", \"verdaderamente\",\n",
    "                \"bien\", \"super\", \"supel\", \"re\"]\n",
    "\n",
    "        apocope_dict = {'gran': 'grande',\n",
    "                        'buen': 'bueno',\n",
    "                        'cien': 'ciento',\n",
    "                        'cualquier': 'cualquiero',\n",
    "                        'cualesquier': 'cualesquiero',\n",
    "                        'algún': 'alguno',\n",
    "                        'mal': 'malo',\n",
    "                        'ningún': 'ninguno',\n",
    "                        'primer': 'primero',\n",
    "                        'tercer': 'tercero',\n",
    "                        'un': 'uno'}\n",
    "\n",
    "        forms_of_estar = [\"estar\", \"estal\",\n",
    "                          \"estoy\", \"estás\", \"está\", \"estamos\", \"estáis\", \"están\",\n",
    "                          \"estaba\", \"estabas\", \"estábamos\", \"estabais\", \"estaban\",\n",
    "                          \"estuve\", \"estuviste\", \"estuvo\", \"estuvimos\", \"estuvisteis\", \"estuvieron\",\n",
    "                          \"estaré\", \"estarás\", \"estará\", \"estaremos\", \"estaréis\", \"estarán\",\n",
    "                          \"estaría\", \"estarías\", \"estaría\", \"estaríamos\", \"estaríais\", \"estarían\",\n",
    "                          \"esté\", \"estés\", \"estemos\", \"estéis\", \"estén\",\n",
    "                          \"estuviera\", \"estuvieras\", \"estuviéramos\", \"estuvierais\", \"estuvieran\",\n",
    "                          \"estuviese\", \"estuvieses\", \"estuviésemos\", \"estuvieseis\", \"estuviesen\",\n",
    "                          \"estuviere\", \"estuvieres\", \"estuviéremos\", \"estuviereis\", \"estuvieren\",\n",
    "                          \"estad\"]\n",
    "\n",
    "        forms_of_ser = [\"ser\", \"sel\",\n",
    "                        \"soy\", \"eres\", \"es\", \"somos\", \"sois\", \"son\",\n",
    "                        \"era\", \"eras\", \"éramos\", \"erais\", \"eran\",\n",
    "                        \"fui\", \"fuiste\", \"fue\", \"fuimos\", \"fuisteis\", \"fueron\",\n",
    "                        \"seré\", \"serás\", \"será\", \"seremos\", \"seréis\", \"serán\",\n",
    "                        \"sería\", \"serías\", \"sería\", \"seríamos\", \"seríais\", \"serían\",\n",
    "                        \"sea\", \"seas\", \"seás\", \"seamos\", \"seáis\", \"sean\",\n",
    "                        \"fuera\", \"fueras\", \"fuéramos\", \"fuerais\", \"fueran\",\n",
    "                        \"fuese\", \"fueses\", \"fuésemos\", \"fueseis\", \"fuesen\",\n",
    "                        \"fuere\", \"fueres\", \"fuéremos\", \"fuereis\", \"fueren\",\n",
    "                        \"sé\", \"sed\"]\n",
    "\n",
    "        forms_of_parecer = [\"parecer\", \"parecel\",\n",
    "                            \"parezco\", \"pareces\", \"parecés\", \"parece\", \"parecemos\", \"parecéis\", \"parecen\",\n",
    "                            \"parecía\", \"parecías\", \"parecíamos\", \"parecíais\", \"parecían\",\n",
    "                            \"parecí\", \"pareciste\", \"pareció\", \"parecimos\", \"parecisteis\", \"parecieron\",\n",
    "                            \"pareceré\", \"parecerás\", \"parecerá\", \"pareceremos\", \"pareceréis\", \"parecerán\",\n",
    "                            \"parecería\", \"parecerías\", \"parecería\", \"pareceríamos\", \"pareceríais\", \"parecerían\",\n",
    "                            \"parezca\", \"parezcas\", \"parezcás\", \"parezcamos\", \"parezcáis\", \"parezcan\",\n",
    "                            \"pareciera\", \"parecieras\", \"pareciéramos\", \"parecierais\", \"parecieran\",\n",
    "                            \"pareciese\", \"parecieses\", \"pareciésemos\", \"parecieseis\", \"pareciesen\",\n",
    "                            \"pareciere\", \"parecieres\", \"pareciéremos\", \"pareciereis\", \"parecieren\",\n",
    "                            \"paracé\", \"pareced\"]\n",
    "\n",
    "        copulas = forms_of_estar + forms_of_ser + forms_of_parecer\n",
    "\n",
    "        # Get the current working directory in Colab\n",
    "        colab_dir = \"/content\"\n",
    "\n",
    "        # Construct the absolute path to the input folder\n",
    "        input_folder_path = os.path.join(colab_dir, input_folder)\n",
    "\n",
    "        # Create a list to store data frames\n",
    "        data_frames = []\n",
    "\n",
    "        # Iterate over marked files in the input folder\n",
    "        for filename in os.listdir(input_folder_path):\n",
    "            # SPEAKER\n",
    "            pattern = re.compile(r'\\.txt')\n",
    "            speaker = pattern.sub('', \"CART\" + filename[:2])\n",
    "            print(\"Started analyzing:\", speaker)\n",
    "\n",
    "            # Construct the absolute path to the marked file\n",
    "            marked_file_path = os.path.join(input_folder_path, filename)\n",
    "\n",
    "            # Initialize a list to store row data\n",
    "            rows = []\n",
    "\n",
    "            # Read the marked text from the file\n",
    "            with open(marked_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                # Process marked text line by line\n",
    "                for line in file:\n",
    "                    # Split the line into segments based on custom sentence-ending punctuation\n",
    "                    segments = re.split(r'(?<=[,.!?]_PUNCT_[,.!?])', line)\n",
    "\n",
    "                    # Initialize variables inside the inner loop\n",
    "                    intensifier = None\n",
    "                    adjective = None\n",
    "                    lex_adjective = None\n",
    "                    attributive = None\n",
    "                    noun = None\n",
    "                    double_intensified = None\n",
    "\n",
    "                    # Process each segment\n",
    "                    for segment in segments:\n",
    "                        # Split the segment into words\n",
    "                        words = segment.split()\n",
    "\n",
    "                        # Process each word in the segment\n",
    "                        for i, word in enumerate(words):\n",
    "                            if \"_ADJ_\" in word and any(word.endswith(c) for c in 'oaeslndr') and not(word.endswith('mente')) and re.match(r'^[a-zA-Z_áéíóúüñÁÉÍÓÚÜÑ]+$', word):\n",
    "                                # ADJECTIVO\n",
    "                                adjective = re.search(r'^(.*)_ADJ_', word).group(1)\n",
    "                                lex_adjective = re.search(r'_ADJ_(.+)', word).group(1)\n",
    "                                # apocope corrections\n",
    "                                if lex_adjective in apocope_dict:\n",
    "                                    lex_adjective = apocope_dict[lex_adjective]\n",
    "                                if lex_adjective.endswith(\"s\"):\n",
    "                                    lex_adjective = lex_adjective[:-1]\n",
    "                                if lex_adjective.endswith(\"a\"):\n",
    "                                    lex_adjective = lex_adjective[:-1] + \"o\"\n",
    "                                ####print(adjective)\n",
    "                                ####print(lex_adjective)\n",
    "                                # ATRIBUTIVO, NOMINAL\n",
    "                                # noun adj\n",
    "                                if i > 0 and \"_NOUN_\" in words[i-1] and not any(inter in words[i-1] for inter in intensifier_list):\n",
    "                                    attributive = \"attributive\"\n",
    "                                    noun = re.search(r'^(.*?)_', words[i-1]).group(1)\n",
    "                                # adj noun\n",
    "                                elif i < len(words) - 1 and \"_NOUN_\" in words[i+1]:\n",
    "                                    attributive = \"attributive\"\n",
    "                                    noun = re.search(r'^(.*?)_', words[i+1]).group(1)\n",
    "                                # noun int adj\n",
    "                                elif i > 1 and \"_NOUN_\" in words[i-2] and any(inter in words[i-1] for inter in intensifier_list):\n",
    "                                    attributive = \"attributive\"\n",
    "                                    noun = re.search(r'^(.*?)_', words[i-2]).group(1)\n",
    "                                # cop int adj\n",
    "                                elif i > 1 and any(form in words[i-2] for form in copulas):\n",
    "                                    attributive = \"predicative\"\n",
    "                                    noun = \"/\"\n",
    "                                # cop adj\n",
    "                                elif i > 0 and any(form in words[i-1] for form in copulas):\n",
    "                                    attributive = \"predicative\"\n",
    "                                    noun = \"/\"\n",
    "                                else:\n",
    "                                    attributive = \"ambiguous\"\n",
    "                                    noun = \"/\"\n",
    "                                if not re.match(r'^[a-zA-Z_áéíóúüñÁÉÍÓÚÜÑ/]+$', noun):\n",
    "                                    attributive = \"ambiguous\"\n",
    "                                    noun = \"/\"\n",
    "                                ####print(noun)\n",
    "                                # INTENSIFICADOR\n",
    "                                if i > 0:\n",
    "                                    prev_word_match = re.search(r'^(.*?)_', words[i-1])\n",
    "                                    if prev_word_match:\n",
    "                                        prev_word = prev_word_match.group(1)\n",
    "                                        if prev_word in intensifier_list:\n",
    "                                            intensifier = prev_word\n",
    "                                        else:\n",
    "                                            intensifier = '/'\n",
    "                                    else:\n",
    "                                        intensifier = '/'\n",
    "                                else:\n",
    "                                    intensifier = '/'\n",
    "                                if adjective.endswith(\"ísimo\") or adjective.endswith(\"ísima\"):\n",
    "                                    intensifier = '-ísimo'\n",
    "                                if adjective.startswith(\"archi\"):\n",
    "                                    intensifier = 'archi-'\n",
    "                                if intensifier == 'super' or intensifier == 'supel':\n",
    "                                    intensifier = 'súper'\n",
    "                                if adjective.startswith(\"re\"):\n",
    "                                    print(adjective)\n",
    "                                ####print(intensifier)\n",
    "                                # DOBLE_INT\n",
    "                                if intensifier != '/' and i > 1:\n",
    "                                    prev_prev_word_match = re.search(r'^(.*?)_', words[i-2])\n",
    "                                    if prev_prev_word_match:\n",
    "                                        prev_prev_word = prev_prev_word_match.group(1)\n",
    "                                        if prev_prev_word in intensifier_list:\n",
    "                                            double_intensified = prev_prev_word\n",
    "                                        else:\n",
    "                                            double_intensified = '/'\n",
    "                                    else:\n",
    "                                        double_intensified = '/'\n",
    "                                else:\n",
    "                                    double_intensified = '/'\n",
    "                                # FRASE_COMPLETA\n",
    "                                phrase_tagged = ' '.join(words)\n",
    "                                # FRASE REAL\n",
    "                                phrase = ' '.join(re.sub(r'_.*', '', word) for word in words)\n",
    "\n",
    "                                row_data = {\n",
    "                                    'ID': speaker,\n",
    "                                    'Phr_tagged': phrase_tagged,\n",
    "                                    'Phr': phrase,\n",
    "                                    'Int': intensifier,\n",
    "                                    'Double': double_intensified,\n",
    "                                    'Adj': adjective,\n",
    "                                    'Lex_Adj': lex_adjective,\n",
    "                                    'Adj_type': attributive,\n",
    "                                    'Noun': noun,\n",
    "                                }\n",
    "                                rows.append(row_data)\n",
    "\n",
    "                    # Check if rows list is not empty\n",
    "            if rows:\n",
    "                # Create a DataFrame for the current file and append it to the list\n",
    "                df = pd.DataFrame(rows)\n",
    "                data_frames.append(df)\n",
    "                print(f\"Appended DataFrame for file: {filename}\")\n",
    "            else:\n",
    "                print(f\"No valid rows found in file: {filename}\")\n",
    "\n",
    "        # Concatenate all data frames in the list\n",
    "        if data_frames:\n",
    "            data = pd.concat(data_frames, ignore_index=True)\n",
    "            # Save the DataFrame to a CSV file\n",
    "            data.to_csv(input_path + \"\\\\temp.csv\", index=False)\n",
    "            print(\"Successfully concatenated data frames\")\n",
    "        else:\n",
    "            print(\"No data frames to concatenate\")\n",
    "\n",
    "            # Create a DataFrame for the current file and append it to the list\n",
    "            data_frames.append(pd.DataFrame(rows))\n",
    "\n",
    "        # Concatenate all data frames in the list\n",
    "        data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        data.to_csv(OUTPUT_FILE_NAME, index=False)\n",
    "\n",
    "    def closest_verb_non_infinitive(row):\n",
    "        phrase = row['Phr_tagged']\n",
    "        intensifier_pos = phrase.find(row['Adj'])\n",
    "\n",
    "        # Initialize minimum distances and closest matches\n",
    "        min_distance_verb = float('inf')\n",
    "        min_distance_aux = float('inf')\n",
    "        closest_verb = None  # To store the closest verb\n",
    "        closest_aux = None   # To store the closest auxiliary\n",
    "\n",
    "        # Regular expression to find verb and auxiliary patterns\n",
    "        pattern = r'\\b(\\w+)_(VERB|AUX)_(\\w+)\\b'  # Capture the first word\n",
    "\n",
    "        # Use re.finditer to find all matches of the pattern\n",
    "        for match in re.finditer(pattern, phrase):\n",
    "            # Find the start and end of the match\n",
    "            start_pos = match.start()\n",
    "            end_pos = match.end()\n",
    "\n",
    "            # Calculate distance as the distance to the center of the match\n",
    "            match_center = (start_pos + end_pos) // 2\n",
    "            distance = abs(match_center - intensifier_pos)\n",
    "\n",
    "            # Check if the match is VERB or AUX and update accordingly\n",
    "            if 'VERB' in match.group():\n",
    "                if distance < min_distance_verb:\n",
    "                    min_distance_verb = distance\n",
    "                    closest_verb = match.group(1)  # Store the first word\n",
    "            elif 'AUX' in match.group():\n",
    "                if distance < min_distance_aux:\n",
    "                    min_distance_aux = distance\n",
    "                    closest_aux = match.group(1)  # Store the first word\n",
    "\n",
    "        # Determine which is closer to the intensifier\n",
    "        if closest_verb and closest_aux:\n",
    "            if min_distance_verb < min_distance_aux:\n",
    "                return closest_verb\n",
    "            else:\n",
    "                return closest_aux\n",
    "        elif closest_verb:\n",
    "            return closest_verb\n",
    "        elif closest_aux:\n",
    "            return closest_aux\n",
    "        else:\n",
    "            return '/'  # Return '/' if no relevant forms are found\n",
    "\n",
    "    def closest_verb_infinitive(row):\n",
    "        phrase = row['Phr_tagged']\n",
    "        intensifier_pos = phrase.find(row['Adj'])\n",
    "\n",
    "        # Initialize minimum distances and closest matches\n",
    "        min_distance_verb = float('inf')\n",
    "        min_distance_aux = float('inf')\n",
    "        closest_verb = None  # To store the closest verb\n",
    "        closest_aux = None   # To store the closest auxiliary\n",
    "\n",
    "        # Regular expression to find verb and auxiliary patterns\n",
    "        pattern = r'\\b(\\w+)_(VERB|AUX)_(\\w+)\\b'  # Capture both words\n",
    "\n",
    "        # Use re.finditer to find all matches of the pattern\n",
    "        for match in re.finditer(pattern, phrase):\n",
    "            # Find the start and end of the match\n",
    "            start_pos = match.start()\n",
    "            end_pos = match.end()\n",
    "\n",
    "            # Calculate distance as the distance to the center of the match\n",
    "            match_center = (start_pos + end_pos) // 2\n",
    "            distance = abs(match_center - intensifier_pos)\n",
    "\n",
    "            # Check if the match is VERB or AUX and update accordingly\n",
    "            if 'VERB' in match.group():\n",
    "                if distance < min_distance_verb:\n",
    "                    min_distance_verb = distance\n",
    "                    closest_verb = match.group(3)  # Store the second word (infinitive)\n",
    "            elif 'AUX' in match.group():\n",
    "                if distance < min_distance_aux:\n",
    "                    min_distance_aux = distance\n",
    "                    closest_aux = match.group(3)  # Store the second word (infinitive)\n",
    "\n",
    "        # Determine which is closer to the intensifier\n",
    "        if closest_verb and closest_aux:\n",
    "            if min_distance_verb < min_distance_aux:\n",
    "                return closest_verb\n",
    "            else:\n",
    "                return closest_aux\n",
    "        elif closest_verb:\n",
    "            return closest_verb\n",
    "        elif closest_aux:\n",
    "            return closest_aux\n",
    "        else:\n",
    "            return '/'  # Return '/' if no relevant forms are found\n",
    "\n",
    "    remove_SPACE()\n",
    "\n",
    "    # Call the function with the marked input folder path\n",
    "    analyze(INPUT_FOLDER_NAME)\n",
    "\n",
    "    # Read the original CSV file into a DataFrame\n",
    "    df = pd.read_csv(OUTPUT_FILE_NAME)\n",
    "\n",
    "    # Apply the function to create the new column\n",
    "    df['Verb'] = df.apply(closest_verb_non_infinitive, axis=1)\n",
    "    df['Inf_Verb'] = df.apply(closest_verb_infinitive, axis=1)\n",
    "\n",
    "    # Save the modified DataFrame back to a CSV and XLSX\n",
    "    df.to_csv(OUTPUT_FILE_NAME, index=False)\n",
    "    df = pd.read_csv(OUTPUT_FILE_NAME)\n",
    "    output_file_name = OUTPUT_FILE_NAME[:-4] + '.xlsx'\n",
    "    df.to_excel(output_file_name, index=False)\n",
    "\n",
    "    # Check if the file exists before trying to delete it\n",
    "    os.remove(input_path + \"\\\\\" + output_filename + \".csv\")\n",
    "    os.remove(input_path + \"\\\\temp.csv\")\n",
    "\n",
    "    print(\"Successfully created data\")\n",
    "create_data(\"C:\\\\ClassStuff\\\\4005\\\\datos\", \"intensifier_data_CART_101815\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f2a231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_data = \"..\\\\datos\\\\intensifier_data_CART_101815.xlsx\"\n",
    "\n",
    "cart_df = pd.read_excel(cart_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make numbered based on speaker col\n",
    "def add_number(df, output_file):\n",
    "    def numero(row, count_dict={}):\n",
    "        # Split the original label to get the speaker number\n",
    "        speaker_number = row['ID']\n",
    "\n",
    "        # Update the count for this speaker number\n",
    "        count_dict[speaker_number] = count_dict.get(speaker_number, 0) + 1\n",
    "\n",
    "        # Generate the new label\n",
    "        new_label = f'{speaker_number}_{count_dict[speaker_number]:02d}'\n",
    "        return new_label\n",
    "    df['Number'] = df.apply(numero, axis=1)\n",
    "    df.to_excel(output_file)\n",
    "\n",
    "add_number(cart_df, cart_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d49dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged words saved to ..\\datos\\intensifier_data_CART_101815.xlsx\n"
     ]
    }
   ],
   "source": [
    "# fix -o to o|a|s etc.\n",
    "def lex_adj_fixer(df, output_file):\n",
    "    # Assuming the column name is 'word', adjust if necessary\n",
    "    words = df['Adj'].tolist()\n",
    "\n",
    "    # Dictionaries to store grouped words\n",
    "    word_groups = defaultdict(set)\n",
    "    a_groups = defaultdict(set)\n",
    "    e_groups = defaultdict(set)\n",
    "    es_groups = defaultdict(set)\n",
    "    word_counts = defaultdict(int)\n",
    "\n",
    "    # Identify and group words by root while counting occurrences\n",
    "    for word in words:\n",
    "        word_counts[word] += 1  # Track occurrences\n",
    "\n",
    "        match_o_a = re.match(r\"(.+?)(o|a|os|as)$\", word)\n",
    "        match_a = re.match(r\"(.+?)(es|a|as)$\", word)\n",
    "        match_e = re.match(r\"(.+?)(e|es)$\", word)\n",
    "        match_es = re.match(r\"(.+?)(es)$\", word)\n",
    "\n",
    "        if match_o_a:\n",
    "            root, ending = match_o_a.groups()\n",
    "            word_groups[root].add(ending)\n",
    "        elif match_a:\n",
    "            root, ending = match_a.groups()\n",
    "            word_groups[root].add(ending)\n",
    "        elif match_e:\n",
    "            root, ending = match_e.groups()\n",
    "            e_groups[root].add(ending)\n",
    "        elif match_es:\n",
    "            root, ending = match_es.groups()\n",
    "            es_groups[root].add(ending)\n",
    "        else:\n",
    "            es_groups[word].add(\"\")  # Mark singular words\n",
    "\n",
    "    # Dictionary to store final merged word mappings\n",
    "    merged_map = {}\n",
    "\n",
    "    # Process \"o, a, os, as\" groups\n",
    "    for root, endings in word_groups.items():\n",
    "        sorted_endings = sorted(endings)\n",
    "        merged = root + \"|\".join(sorted_endings) if sorted_endings else root\n",
    "        merged_map.update({root + ending: merged for ending in endings})  # Map all variations to merged form\n",
    "\n",
    "    # Process \"es, a, as\" groups\n",
    "    for root, endings in a_groups.items():\n",
    "        sorted_endings = sorted(endings)\n",
    "        merged = root + \"|\".join(sorted_endings) if sorted_endings else root\n",
    "        merged_map.update({root + ending: merged for ending in endings})  # Map all variations to merged form\n",
    "\n",
    "    # Process \"e, es\" groups\n",
    "    for root, endings in e_groups.items():\n",
    "        merged = f\"{root}e|s\" if endings == {\"e\", \"es\"} else f\"{root}{'|'.join(sorted(endings))}\"\n",
    "        merged_map.update({root + ending: merged for ending in endings})\n",
    "\n",
    "    # Process singular (\"\") and plural (\"es\") groups\n",
    "    for root, endings in es_groups.items():\n",
    "        if endings == {\"\", \"es\"}:\n",
    "            merged = f\"{root}|es\"\n",
    "        elif endings == {\"es\"}:\n",
    "            merged = f\"{root}es\"\n",
    "        else:\n",
    "            merged = root\n",
    "        merged_map[root] = merged\n",
    "        merged_map[root + \"es\"] = merged  # Handle plural form\n",
    "\n",
    "    # Generate final expanded list\n",
    "    expanded_merged_words = []\n",
    "    for word in words:\n",
    "        expanded_merged_words.append(merged_map.get(word, word))  # Replace with merged form\n",
    "\n",
    "    # Save to Excel\n",
    "    output_df = pd.DataFrame({\"merged_word\": expanded_merged_words})\n",
    "    output_df.to_excel(output_file, index=False)  # Save as Excel file\n",
    "\n",
    "    print(f\"Merged words saved to {output_file}\")\n",
    "\n",
    "lex_adj_fixer(cart_df, cart_data) # NOT USED RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca38a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th bueno: 88 (11.91%)\n",
      "2th primero: 31 (4.19%)\n",
      "3th único: 20 (2.71%)\n",
      "4th mejor: 16 (2.17%)\n",
      "5th mismo: 16 (2.17%)\n",
      "6th grande: 15 (2.03%)\n",
      "7th bonito: 13 (1.76%)\n",
      "8th pequeño: 12 (1.62%)\n",
      "9th normal: 12 (1.62%)\n",
      "10th último: 12 (1.62%)\n",
      "11th económico: 11 (1.49%)\n",
      "12th malo: 10 (1.35%)\n",
      "13th solo: 9 (1.22%)\n",
      "14th cristiano: 9 (1.22%)\n",
      "15th segundo: 8 (1.08%)\n"
     ]
    }
   ],
   "source": [
    "# lex adjs by freq\n",
    "def lex_adjs_freq(df):\n",
    "    # Assuming the column of words is named 'words'\n",
    "    words = df['Lex_Adj'].dropna().tolist()  # Remove NaNs and convert to list\n",
    "\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = sum(word_counts.values())\n",
    "\n",
    "    # Create a sorted list of words with their percentages\n",
    "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print results\n",
    "    place = 0\n",
    "    for word, count in sorted_word_counts:\n",
    "        percentage = (count / total_words) * 100\n",
    "        if percentage >= 1.0: ## ! CHANGE HERE TO CHANGE RANGE SHOWN'\n",
    "            place += 1\n",
    "            print(f\"{place}th {word}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "lex_adjs_freq(cart_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7b1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add syl col\n",
    "def count_syllables(word):\n",
    "    # Remove silent 'u' in gue/gui/que/qui unless it has a diaeresis (ü)\n",
    "    word = re.sub(r'gue|gui|que|qui', lambda m: m.group(0).replace('u', ''), word)\n",
    "\n",
    "    # Count vowel groups (as a simple approximation of syllables)\n",
    "    syllables = re.findall(r'[aeiouáéíóúü]+', word)\n",
    "    \n",
    "    return len(syllables)\n",
    "\n",
    "def addSyllableCol(df):\n",
    "    # Create a new column for the number of syllables in the text\n",
    "    # Does not account for diphthongs or triphthongs\n",
    "    df['Syllable'] = df['Adj'].apply(count_syllables)\n",
    "    return df\n",
    "\n",
    "df = addSyllableCol(cart_df)\n",
    "df.to_excel(cart_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
